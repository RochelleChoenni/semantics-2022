---
layout: post
title: Tips for Practical 2
date:   2018-05-03
author: Wilker
categories: Info
mathjax: true
---

Hello everyone,

I decided to summarise some of the tips that were discussed in the previous two labs. 

# BSG

In the BSG model, we have a parameterised prior that conditions on the central word $$w$$:

\begin{equation}
Z|w \sim \mathcal N(\mu_w, \sigma^2_w)
\end{equation}

To get it right we need to have a matrix of locations ($$L$$) and a matrix of scale ($$S$$) vectors:

* $$L$$ has shape `[emb_dim, vocab_size]` and is initialised at random
* $$S = \text{softplus}(\hat S)$$ where $$\hat S$$ has shape `[emb_dim, vocab_size]` and is initialised at random: note that I apply softplus to make sure scales are strictly positive 

Throughout, let the function `onehot(x)` returns a one-hot encoding of a word x (thus the result of `onehot(x)` has shape `[vocab_size, 1]`).

**Generative model**

To get a prior location and scale for a central word $$w$$ we basically use matrix multiplication:

* prior location: $$\mu_w = \text{matmul}(L, \text{onehot}(w))$$
* prior scale: $$\sigma_w = \text{matmul}(S, \text{onehot}(w))$$

The *decoder* must obtain, for each central word $$w$$ and (sampled) embedding $$z$$, the parameters of a Categorical distribution over the vocabulary of the language (that is a `vocab_size`-dimensional probability vector):

\begin{equation}
C_j|w,z \sim \text{Cat}(\mathbf f_w)
\end{equation}

This distribution is used to assign a likelihood 
$$P(c_j|w,z)$$ 
to context words given a central word $$w$$ and its (sampled) embedding $$z$$.

The vector $$\mathbf f_w $$ is computed via a simple feedforward NN:
$$\mathbf f_w =  \text{softmax}(\text{affine1}(z))$$ 

**Inference model**

Let $$R$$ denote some deterministic embeddings, thus $$R$$ has shape `[emb_dim, vocab_size]`. 
In the paper, the authors 

1. Concatenate a deterministic representation of each context word $$c_j$$ with a deterministic representation of the central word $$w$$: $$\text{concat}(\text{matmul}(R, \text{onehot}(c_j)), \text{matmul}(R, \text{onehot}(w)))$$. 
2. To the result, they apply a projection $$M$$ followed by a ReLU. 
3. Then, they aggregate all vectors using elementwise sum. 
4. To the resulting vector, they apply an affine layer for the variational location and another affine layer with softplus activation for the variational scale.

**ELBO** 

Just get a reparameterised sample from the inference model, stick that into the ``decoder'' and build the loss (negative ELBO), all terms are now available. 

**Softmax approximation**

If you find it difficult to implement the softmax approximation, you can ignore it, instead using a smaller vocabulary by mapping stop words to UNK as well as infrequent words. 


# EmbedAlign

**Generative model**

* Prior location: 0 (fixed)
* Prior scale: 1 (fixed)

Distribution over vocabulary of L1: $$\mathbf f_i = \text{softmax}(\text{affine1}(z_i))$$ for every $$i = 1..m$$. Note that this returns a vector of L1 vocabulary-size per word in $$x_1^m$$.

Distribution over vocabulary of L2: $$\mathbf g_i = \text{softmax}(\text{affine2}(z_i))$$ for every $$i = 1..m$$. Note that this returns a vector of L2 vocabulary-size per word in $$x_1^m$$. You did not read it wrong, it's per word in $$x_1^m$$, not in $$y_1^n$$.

Alignment distribution parameter: 1 / m.


**Inference model**

Embed words deterministically using a matrix $$E$$ with shape `[emb_dim, vocab_size]`: that is, $$\mathbf h_i = \text{matmul}(E, \text{onehot}(x_i))$$.

If applicable re-encode the words using a BiLSTM: $$\mathbf h_1^m =\text{bilstm}(\mathbf h_1^m)$$.

Variational location: $$\mathbf u_i = \text{affine3}(z_i)$$ for each $$i=1..m$$.

Variational scale: $$\mathbf s_i = \text{softplus}(\text{affine4}(z_i))$$ for each $$i=1..m$$.

This will give you $$m$$ Gaussians, one per word in $$x_1^m$$. Then you can obtain 1 sample per position and build the loss. 

**ELBO**

The loss (negative ELBO) will have a term  

$$\log P(x_i|z_i)$$ 

per word in $$x_1^m$$. 
This is the log of the entry in $$\mathbf f_i$$ that corresponds to $$x_i$$.

It will also have a term 

$$\log P(y_i|z_1^m)$$ 

per word in $$y_1^n$$. This however requires a marginalisation.

\begin{equation}
    P(y_i|z_1^m) = \sum_{a_j=1}^m P(a_j|m) P(y_j|z_{a_j})
\end{equation}

This basically iterates for every $$a_j$$ from 1 to $$m$$ selecting the probability corresponding to $$y_j$$ had it been aligned to a certain position $$a_j$$ in $$x_1^m$$. To compute it, you basically have to average the probabilities of $$y_j$$ in $$\mathbf g_1$$, $$\mathbf g_2$$, all the way to $$\mathbf g_m$$. Recall that the alignment probability is uniform (1/m), and that's why this is just an average. 

Finally, in the loss there will be 1 KL term per word in $$x_1^m$$, which are all analytically computable. 


**Softmax approximation**

Again, if you find it difficult to implement the softmax approximation, you can ignore it, instead using a smaller vocabulary by mapping stop words to UNK as well as infrequent words. The same strategy applies to both languages in EmbedAlign. 



